{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate importing multiple CSV files into Google BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "- import the CSV file into a pandas df\n",
    "- create a function to clean the table name and remove all extra symbols, spaces, capital letters\n",
    "- create a function to clean the column headers and remove all extra symbols, spaces, capital letters\n",
    "- import the data into the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from pandas.io import gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_files():\n",
    "\n",
    "    #get names of only csv files\n",
    "    csv_files = []\n",
    "    for file in os.listdir(os.getcwd()):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_files.append(file)\n",
    "\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "def configure_dataset_directory(csv_files, dataset_dir):\n",
    "  \n",
    "    #make dataset folder to process csv files\n",
    "    try: \n",
    "      mkdir = 'mkdir {0}'.format(dataset_dir)\n",
    "      os.system(mkdir)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    #move csv files to dataset folder\n",
    "    for csv in csv_files:\n",
    "      mv_file = \"mv '{0}' {1}\".format(csv, dataset_dir)\n",
    "      os.system(mv_file)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def create_df(dataset_dir, csv_files):\n",
    "  \n",
    "    data_path = os.getcwd()+'/'+dataset_dir+'/'\n",
    "\n",
    "    #loop through the files and create the dataframe\n",
    "    df = {}\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df[file] = pd.read_csv(data_path+file)\n",
    "        except UnicodeDecodeError:\n",
    "            df[file] = pd.read_csv(data_path+file, encoding=\"ISO-8859-1\") #if utf-8 encoding error\n",
    "        print(file)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_tbl_name(filename):\n",
    "  \n",
    "    #rename csv, force lower case, no spaces, no dashes\n",
    "    clean_tbl_name = filename.lower().replace(\" \", \"\").replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"$\",\"\").replace(\"%\",\"\")\n",
    "    \n",
    "    tbl_name = '{0}'.format(clean_tbl_name.split('.')[0])\n",
    "\n",
    "    return tbl_name\n",
    "\n",
    "\n",
    "\n",
    "def clean_colname(dataframe):\n",
    "  \n",
    "    #force column names to be lower case, no spaces, no dashes\n",
    "    dataframe.columns = [x.lower().replace(\" \", \"_\").replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\".\",\"_\").replace(\"$\",\"\").replace(\"%\",\"\") for x in dataframe.columns]\n",
    "    \n",
    "    #processing data\n",
    "    replacements = {\n",
    "        'timedelta64[ns]': 'varchar',\n",
    "        'object': 'varchar',\n",
    "        'float64': 'float',\n",
    "        'int64': 'int',\n",
    "        'datetime64': 'timestamp'\n",
    "    }\n",
    "\n",
    "    col_str = \", \".join(\"{} {}\".format(n, d) for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(replacements)))\n",
    "    \n",
    "    return col_str, dataframe.columns\n",
    "\n",
    "def upload_to_db(df,tbl_name):\n",
    "    df.to_gbq(destination_table= dataset+'.'+ tbl_name,project_id= project_id, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Contracts$.csv\n",
      "Customer Engagements.csv\n",
      "Customer Demo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2319.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1715.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2118.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "#settings\n",
    "dataset_dir = 'datasets'\n",
    "project_id = 'csv-upload-358620' # add project id here\n",
    "dataset = 'csv_upload' # add dataset name here\n",
    "\n",
    "\n",
    "\n",
    "#configure environment and create main df\n",
    "csv_files = csv_files()\n",
    "configure_dataset_directory(csv_files, dataset_dir)\n",
    "df = create_df(dataset_dir, csv_files)\n",
    "\n",
    "for k in csv_files:\n",
    "\n",
    "    #call dataframe\n",
    "    dataframe = df[k]\n",
    "\n",
    "    #clean table name\n",
    "    tbl_name = clean_tbl_name(k)\n",
    "    \n",
    "    #clean column names\n",
    "    col_str, dataframe.columns = clean_colname(dataframe)\n",
    "    \n",
    "    #upload to GBQ\n",
    "    upload_to_db(dataframe,tbl_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
